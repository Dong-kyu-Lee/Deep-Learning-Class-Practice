{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T03:53:07.862391Z",
     "start_time": "2025-09-10T03:53:05.048648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "===============================\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "===============================\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# ValueError: expected sequence of length 3 at dim 3 (got 2)\\n# 3번째 디맨션의 길이가 일치하지 않음. [1,2,3]=3개 vs [4,5]=2개\\na11 = torch.tensor([\\n    [[[1, 2, 3], [4, 5]]],\\n    [[[1, 2, 3], [4, 5]]],\\n    [[[1, 2, 3], [4, 5]]],\\n    [[[1, 2, 3], [4, 5]]],\\n])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.Tensor class 를 사용해 텐서 생성\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])\n",
    "\n",
    "# gpu가 달려있다면, 아래 코드를 통해 t1과 동일한 텐서를 gpu에 저장함.\n",
    "# t1_cuda = t1.to(torch.device('cuda')) 또는\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()\n",
    "\n",
    "print(\"===============================\")\n",
    "\n",
    "# torch.tensor function\n",
    "t2 = torch.tensor([1,2,3], device='cpu')\n",
    "print(t2.dtype) # >>> torch.int64 <-- Tensor class랑 다름!\n",
    "print(t2.device)\n",
    "print(t2.requires_grad)\n",
    "print(t2.size())\n",
    "print(t2.shape)\n",
    "\n",
    "# Tensor class 사용할 때와 마찬가지로\n",
    "# t2_cuda = t2.cuda() 이런게 가능하다.\n",
    "t2_cup = t2.cpu()\n",
    "\n",
    "print(\"===============================\")\n",
    "\n",
    "a1 = torch.tensor(1)\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1,2,3,4,5])\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1],[2],[3],[4],[5]])\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4],\n",
    "    [5,6],\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a11.shape, a11.ndim)\n",
    "\n",
    "'''\n",
    "# ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "# 3번째 디맨션의 길이가 일치하지 않음. [1,2,3]=3개 vs [4,5]=2개\n",
    "a11 = torch.tensor([\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7a92a-d175-41a6-b937-294bce8ff2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
